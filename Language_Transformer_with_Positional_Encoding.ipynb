{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ebe778-1529-4382-9ffe-e9449f565bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from IPython.core.display_functions import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import wandb\n",
    "import re\n",
    "from tqdm import tqdm,trange\n",
    "device = \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps:0\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\" \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f2a43-8f22-4318-94a8-ff83481b7596",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "\n",
    "print(\"----Sample Shakespeare----\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9de5b-9909-4c6f-b967-a5c765027e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_words(text):\n",
    "    return re.findall(r\"\\w+(?:'\\w+)?|[.,!?;:\\\"()\\[\\]{}<>\\\\/\\-—–…]|\\n\", text)\n",
    "\n",
    "vocab = list(set(split_to_words(text)))\n",
    "c = len(vocab)\n",
    "print(\"Number of words: {}\".format(len(split_to_words(text))))\n",
    "print(\"Number of distinct words in text: {}\".format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ede02-6133-451c-82af-922fac32f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {word:i for i, word in enumerate(vocab)}\n",
    "itos = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "def words_to_tokens(words):\n",
    "    \"\"\"\n",
    "    Convert a list of words to a list of tokens\n",
    "    \"\"\"\n",
    "    return [stoi[w] for w in words]\n",
    "\n",
    "def tokens_to_words(index_list):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to a list of words\n",
    "    \"\"\"\n",
    "    decoded = \" \".join([itos[i] for i in index_list])\n",
    "    return re.sub(r'\\s+([.,!?;:\"(){}\\[\\]<>\\\\/\\-—–…])', r'\\1', decoded)\n",
    "\n",
    "# Checking that the word to token and back conversion works\n",
    "sample_words = text[:36]\n",
    "token_ids = words_to_tokens(split_to_words(sample_words))\n",
    "recovered_words = tokens_to_words(token_ids)\n",
    "print(f\"Original text: {sample_words}\\n\")\n",
    "print(f\"Encoded text: {token_ids}\\n\")\n",
    "print(f\"Recovered text: {recovered_words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32632228-a19d-4fef-a089-49a9af78b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = words_to_tokens(split_to_words(text))\n",
    "print(\"Encoded text sample: {}\".format(tokenized_text[:10]))\n",
    "print(tokens_to_words(tokenized_text[:10]))\n",
    "\n",
    "# The works of Shakespeare are now a sequence of integers representing the words in the text. Sorry, William.\n",
    "tokenized_text = torch.tensor(tokenized_text)\n",
    "tokenized_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071aa732-5e2c-4d2d-970f-b629d04bb2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create co-occurrence matrix\n",
    "# The co-occurrence matrix C is a c x c (c is our vocab size) symmetric matrix where C_ij is how many times the ith word appears within W words away from the jth word.\n",
    "with torch.no_grad():\n",
    "    W = 10\n",
    "    C = torch.zeros(len(vocab),len(vocab))\n",
    "    for t_idx in trange(len(tokenized_text)):\n",
    "        left_bound = max(t_idx-W//2,0)\n",
    "        right_bound = min(t_idx+W//2+1,len(tokenized_text))\n",
    "        context_words = tokenized_text[left_bound : right_bound]\n",
    "        for u_idx in range(left_bound, right_bound):\n",
    "            t = tokenized_text[t_idx]\n",
    "            u = tokenized_text[u_idx]\n",
    "            C[t, u] += 1.0\n",
    "    C = C.to(device)\n",
    "    \n",
    "# C should be a symmetric matrix\n",
    "torch.isclose(C, C.T, atol=1e-3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5ea67d-f775-4364-b405-4a7c1ea13e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n is the number of eigenvectors we want to keep\n",
    "n = 256\n",
    "with torch.no_grad():\n",
    "    # Normalize the data\n",
    "    Z = C - C.mean(dim=1, keepdim=True)\n",
    "    Z /= Z.std(dim=1, keepdim=True)\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    cov = (Z @ Z.T)/(Z.shape[0] - 1)\n",
    "    # Compute the eigenvectors and eigenvalues\n",
    "    L, Q = torch.linalg.eigh(cov)\n",
    "    # Get the n largest eigenvectors\n",
    "    principal_eigv = Q[:, -n:].T\n",
    "\n",
    "    # PCA embeddings for training\n",
    "    pca_embeddings = Z @ principal_eigv.T # (c, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d6a60-5ef2-40cf-a16b-0bc1ac8165b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as the MultiheadLayer in the lab 6 notebook. It corresponds to the equations in Section 3 of this lab's writeup.\n",
    "class MultiHeadLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of the multihead attention layer.\n",
    "    The difference between AttentionLayer and this class is,\n",
    "    now Q,K,V are matrices of shape (H, m, n), and the attention matrix B is of shape (H, T, T)\n",
    "    (one attention feature per head)\n",
    "    Args:\n",
    "        m (int): The dimension of the Q and K matrices.\n",
    "        n (int): The number of features, n=12 in our case.\n",
    "        k (int): The dimension of the W matrix.\n",
    "        H (int): The number of heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, H):\n",
    "        super(MultiHeadLayer, self).__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "\n",
    "        self.Q = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.K = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.V = nn.Parameter(torch.empty(H, m, n))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(H, n, m))\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the values of the learnable parameter matrices.\n",
    "        Kaiming uniform is just a type of random initialization, you don't need to \n",
    "        worry about it. It is a good default initialization for linear layers.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (torch.Tensor): The input embeddings.\n",
    "        Returns:\n",
    "            X_l (torch.Tensor): The output of the multihead attention layer.\n",
    "        \"\"\"\n",
    "        B, n, T = X.shape  # X: (B, n, T)\n",
    "\n",
    "        # Expand X to include the head dimension\n",
    "        X_expanded = X.unsqueeze(1)  # (B, 1, n, T)\n",
    "\n",
    "        # Compute QX, KX, VX for each head\n",
    "        QX = torch.matmul(self.Q.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        KX = torch.matmul(self.K.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        VX = torch.matmul(self.V.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        \n",
    "        QX_t = QX.transpose(-2, -1)  # (B, H, T, m)\n",
    "\n",
    "        # Compute attention scores B per head\n",
    "        B = torch.matmul(QX_t, KX)  # (B, H, T, T)\n",
    "        A = F.softmax(B, dim=-1)\n",
    "    \n",
    "        A_t = A.transpose(-2,-1)\n",
    "        VXA_t = torch.matmul(VX, A_t) # (B, H, m, T)\n",
    "        Y = torch.matmul(self.W, VXA_t) # (B, H, T, n)\n",
    "        \n",
    "        X_l = X + self.nonlinearity(Y.sum(dim=1))\n",
    "\n",
    "        return X_l\n",
    "    \n",
    "    \n",
    "model = MultiHeadLayer(m=32, n=256, H=8).to(device)\n",
    "X_tilde = torch.randn(1,256,64).to(device)\n",
    "out = model(X_tilde)\n",
    "\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d627fd-c016-4bc0-9bd3-231752d97b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language transformer\n",
    "class LanguageTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    Mutlihead Transformer, analogous to the Transformer class, in the single head case.\n",
    "    Args:\n",
    "        m (int): The dimension of the Q and K matrices.\n",
    "        n (int): The number of features, n=12 in our case.\n",
    "        k (int): The dimension of the W matrix.\n",
    "        L (int): The number of layers.\n",
    "        H (int): The number of heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, L, H):\n",
    "        super(LanguageTransformer, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadLayer(m, n, H) for _ in range(L)\n",
    "        ])\n",
    "        # Word embedding table. This is the only change from the previous lab's code. We have \n",
    "        # PCA embeddings to convert word indices to embeddings.\n",
    "        self.embedding_table = pca_embeddings\n",
    "        \n",
    "    def forward(self, E):\n",
    "        \"\"\"\n",
    "        The forward pass of the multihead transformer, stacks L multihead layers.\n",
    "        This class is essentially the same as the Transformer class, but using the \n",
    "        MultiHeadLayer class instead of the AttentionLayer class.\n",
    "        Args:\n",
    "            E (torch.Tensor): The input word indices.\n",
    "        Returns:\n",
    "            X_L^{T-1} (torch.Tensor): The last vector of the output of the transformer.\n",
    "        \"\"\"\n",
    "        # Convert word indices to embeddings. We need to transpose the result to get the shape (B, n, T).\n",
    "        X = self.embedding_table[E].transpose(1,2)\n",
    "        B, n, T = X.shape\n",
    "\n",
    "        # Compute the mean token to append to the sequence.\n",
    "        X_tilde = X.mean(dim=2, keepdim=True) # mean over the time dimension\n",
    "        X_tilde = torch.cat((X, X_tilde), dim=-1)\n",
    "        \n",
    "        # X_l has shape (B, n, T+1)\n",
    "        X_l = X_tilde\n",
    "        for layer in self.layers:\n",
    "            X_l = layer(X_l)\n",
    "        \n",
    "        # Output the last vector.\n",
    "        return X_l[:,:,-1]\n",
    "\n",
    "# Test\n",
    "model = LanguageTransformer(L=2, H=2, m=32, n=256).to(device)\n",
    "E = torch.randint(0, pca_embeddings.shape[0], (1,5)).to(device).long()\n",
    "out = model(E)\n",
    "print(f\"output.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac27872-9e0b-4cf3-aabe-9515f67af7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 64 # context size\n",
    "split_factor = 0.9\n",
    "split_index = int(split_factor * len(tokenized_text))\n",
    "    \n",
    "# Splitting into train and test sets\n",
    "train = tokenized_text[:split_index].to(device)\n",
    "test = tokenized_text[split_index:].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a3b6b-fe73-4c04-a8ca-db11f5f837c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class WordIndexDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This Dataset class takes and encoded tensor of word indices and returns a tensor of context windows of size T.\n",
    "    The tensors returned by this dataset are not yet one-hot encoded.\n",
    "    \"\"\"\n",
    "    def __init__(self, text, T):\n",
    "        self.text = text\n",
    "        self.T = T\n",
    "        assert self.T < len(text), \"context_size (T) must be less than len(text)\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.T\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a single context window of size T. \n",
    "        The context window is a sequence of T words.\n",
    "\n",
    "        During training, we will predict the next token of every word in the context window,\n",
    "        so Y_item is the next word for every word in the context window.\n",
    "        \"\"\"\n",
    "        X_item = self.text[idx:idx + self.T]\n",
    "        Y_item = self.text[idx + 1:idx + self.T + 1]\n",
    "\n",
    "        return X_item, Y_item\n",
    "\n",
    "train_dataset = WordIndexDataset(train, T)\n",
    "test_dataset = WordIndexDataset(test, T)\n",
    "\n",
    "\n",
    "# Example of a batch\n",
    "B = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "E, y_idx = next(iter(train_loader))\n",
    "print(f\"X_idx shape: {E.shape}\")\n",
    "print(f\"y_idx shape: {y_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd5803-8e3f-42af-8282-8f7776930594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "n_epochs = 3\n",
    "m = 32\n",
    "n = 256\n",
    "L = 6\n",
    "T = 64\n",
    "H = 8\n",
    "\n",
    "estimator = LanguageTransformer(m, n, L, H).float().to(device)\n",
    "optimizer = torch.optim.SGD(estimator.parameters(), lr=1e-5)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "cross_entropy_loss = nn.MSELoss()\n",
    "estimator.train()\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(n_epochs): # Iterate over n_epochs epochs\n",
    "\n",
    "    for x_batch, y_batch in tqdm(train_loader): # Iterate over all batches in the dataset \n",
    "        # Load the embeddings for the target word\n",
    "        # We want to predict the last word of the context window for this exercise.\n",
    "        y_word_to_predict = y_batch[:,-1]\n",
    "        Y_embeddings = pca_embeddings[y_word_to_predict].transpose(0,1).to(device) # (B, n)\n",
    "        \n",
    "        # (Step i) Load the data. These commands send the data to the GPU memory.\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # (Step ii) Compute the gradients. We use automated differentiation.\n",
    "        optimizer.zero_grad() # Gradient reset to indicate where the backward computation stops.\n",
    "\n",
    "        # Call the neural network. In this case, we will take the average of the output of the\n",
    "        # transformer as the prediction.\n",
    "        y_hat = estimator(x_batch).mean(dim=-1)\n",
    "        cross_entropy_value = cross_entropy_loss(y_hat,Y_embeddings)\n",
    "\n",
    "        cross_entropy_value.backward() # Compute gradients moving backwards untit the gradient reset.\n",
    "\n",
    "        # (Step iii) Update parameters by taking an SGD (or other optimizer) step.\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(cross_entropy_value.item())\n",
    "    if epoch print(f\"Epoch {epoch}/{n_epochs} Loss: {train_loss[-1]}\")\n",
    "\n",
    "    # End of batch loop.\n",
    "\n",
    "# Evaluate test loss\n",
    "estimator.eval()\n",
    "with torch.no_grad():\n",
    "    test_losses = []\n",
    "    for x_batch, y_batch in tqdm(test_loader):\n",
    "        y_word_to_predict = y_batch[:,-1]\n",
    "        Y_embeddings = pca_embeddings[y_word_to_predict].transpose(0,1).to(device) # (B, n)\n",
    "        y_hat = estimator(x_batch).mean(dim=-1)\n",
    "        test_losses.append(cross_entropy_loss(y_hat,Y_embeddings).item())\n",
    "    test_loss = torch.tensor(test_losses).mean().item()\n",
    "\n",
    "print(f\"Train loss: {train_loss[-1]}\")\n",
    "print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a7d5f-7174-413e-a79e-40dfadf441ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding readout\n",
    "class LanguageTransformerWithReadout(nn.Module):\n",
    "    \"\"\"\n",
    "    A slight modification of the LanguageTransformer class of Task 4.\n",
    "    Args:\n",
    "        m (int): The dimension of the Q and K matrices.\n",
    "        n (int): The number of features, n=12 in our case.\n",
    "        k (int): The dimension of the W matrix.\n",
    "        L (int): The number of layers.\n",
    "        H (int): The number of heads.\n",
    "        c (int): The vocabulary size.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, L, H, c):\n",
    "        super(LanguageTransformerWithReadout, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadLayer(m, n, H) for _ in range(L)\n",
    "        ])\n",
    "\n",
    "        self.embedding_table = pca_embeddings\n",
    "\n",
    "        # Adding readout layer\n",
    "        self.readout = nn.Parameter(torch.empty(c, n).to(device))\n",
    "        nn.init.kaiming_uniform_(self.readout, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, E):\n",
    "        \"\"\"\n",
    "        We change the forward pass from the previous Transformer.\n",
    "        Instead of concatenating a vector to the sequence, we now output a vector of probabilities for each word in the sequence.\n",
    "        Args:\n",
    "            E (torch.Tensor): The input word indices.\n",
    "        Returns:\n",
    "            Y_hat (torch.Tensor): The output of the transformer, passed through the readout layer.\n",
    "        \"\"\"\n",
    "        X = self.embedding_table[E].transpose(1,2)\n",
    "\n",
    "        B, n, T = X.shape\n",
    "        \n",
    "        # X_l has shape (B, n, T+1)\n",
    "        X_l = X\n",
    "        for layer in self.layers:\n",
    "            X_l = layer(X_l)\n",
    "\n",
    "        # We implement the readout layer as a linear mapping on each word in the sequence.\n",
    "        Y_hat = torch.matmul(self.readout, X_l) # (B, c, T)\n",
    "\n",
    "        # Notice, we don't apply the softmax here, because we keep the probabilities unnormalized until \n",
    "        # we call the loss function, for numerical stability.\n",
    "        return Y_hat\n",
    "\n",
    "# testing. Now the transformer outputs a vector of probabilities for each word in the sequence.\n",
    "E = torch.randint(0, len(vocab), (1,5)).to(device).long()\n",
    "model = LanguageTransformerWithReadout(m=32, n=256, L=6, H=8, c=c).to(device)\n",
    "out = model(E)\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73295564-d374-4317-8e56-31c655e9872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "n_epochs = 5\n",
    "B = 64\n",
    "m = 32\n",
    "n = 256\n",
    "L = 6\n",
    "T = 32\n",
    "H = 8\n",
    "\n",
    "estimator = LanguageTransformerWithReadout(m, n, L, H, c).float().to(device)\n",
    "optimizer = torch.optim.SGD(estimator.parameters(), lr=1e-5)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "# We use the Cross Entropy loss for estimating the probabilities of the next word.\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "estimator.train()\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(n_epochs): # Iterate over n_epochs epochs\n",
    "\n",
    "    for x_batch, y_batch in tqdm(train_loader): # Iterate over all batches in the dataset \n",
    "        # We want to predict tha last word of the context window for this exercise.\n",
    "        y_word_to_predict = y_batch[:,-1]\n",
    "        \n",
    "        # Load the embeddings for the words.\n",
    "        X_embeddings = pca_embeddings[x_batch].transpose(1,2) # (B, n, T)\n",
    "        \n",
    "        # (Step i) Load the data. These commands send the data to the GPU memory.\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # (Step ii) Compute the gradients. We use automated differentiation.\n",
    "        optimizer.zero_grad() # Gradient reset to indicate where the backward computation stops.\n",
    "\n",
    "        # Call the neural network. Get the prediction for the last word.\n",
    "        y_hat = estimator(x_batch)[:,:,-1]\n",
    "\n",
    "        # The softmax function is applied internally to the transformer's output y_hat.\n",
    "        cross_entropy_value = cross_entropy_loss(y_hat,y_word_to_predict)\n",
    "\n",
    "        cross_entropy_value.backward() # Compute gradients moving backwards untit the gradient reset.\n",
    "\n",
    "        # (Step iii) Update parameters by taking an SGD (or other optimizer) step.\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(cross_entropy_value.item())\n",
    "\n",
    "    print(f\"Epoch {epoch}/{n_epochs} Loss: {train_loss[-1]}\")\n",
    "\n",
    "    # End of batch loop.\n",
    "# Evaluate test loss at the end of training\n",
    "estimator.eval()\n",
    "with torch.no_grad():\n",
    "    test_losses = []\n",
    "    for x_batch, y_batch in tqdm(test_loader):\n",
    "        y_word_to_predict = y_batch[:,-1]\n",
    "        Y_embeddings = pca_embeddings[y_word_to_predict].transpose(0,1).to(device) # (B, n)\n",
    "        y_hat = estimator(x_batch).mean(dim=-1)\n",
    "        cross_entropy_value = cross_entropy_loss(y_hat,y_word_to_predict)\n",
    "        test_losses.append(cross_entropy_value.item())\n",
    "    test_loss = torch.tensor(test_losses).mean().item()\n",
    "\n",
    "print(f\"Train loss: {train_loss[-1]}\")\n",
    "print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3821d321-fb4f-4b27-a1c8-e44f265d20e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a snippet of the text set to test the model.\n",
    "starting_point = torch.randint(0, len(test)-T, (1,))\n",
    "initial_indices = test[starting_point:starting_point+T].unsqueeze(0)\n",
    "\n",
    "log_probabilities = model(initial_indices)\n",
    "print(f\"log_probabilities.shape: {log_probabilities.shape}\")\n",
    "last_word_probabilities = log_probabilities[:,:,-1]\n",
    "probabilities = F.softmax(last_word_probabilities, dim=-2)\n",
    "\n",
    "print(f\"Input text: {tokens_to_words(initial_indices.reshape(-1).tolist())}\")\n",
    "print(f\"\\nThe most likely next word is: {tokens_to_words([torch.argmax(probabilities).item()])}\")\n",
    "\n",
    "print(\"\\nSampled words according to a multinomial distribution (either could be the next word when using sampling):\")\n",
    "for _ in range(10):\n",
    "    sampled_word = torch.multinomial(probabilities, num_samples=1).item()\n",
    "    print(f\"{tokens_to_words([sampled_word])}\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d947b-360e-4806-9aed-e61ede36b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, X, max_generate_tokens=500):\n",
    "    \"\"\"\n",
    "    Generate text from a model given an initial input token sequence.\n",
    "    Args:\n",
    "        model (nn.Module): The model to use for generation.\n",
    "        input_tokens (torch.Tensor): The initial input token sequence.\n",
    "        max_generate_tokens (int): The maximum number of tokens to generate.\n",
    "    Returns:\n",
    "        torch.Tensor: The generated token sequence.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        context = X.clone()\n",
    "        generated_sequence = X.cpu().squeeze().tolist()  # Ensure it's a 1D list\n",
    "        for _ in range(max_generate_tokens):\n",
    "            logits = model(context)\n",
    "            \n",
    "            last_word_embeddings = logits[:,:,-1]\n",
    "            probs = F.softmax(last_word_embeddings, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Slide context window: remove the first token and append the next token\n",
    "            context = torch.cat([context[:, 1:], next_token], dim=1)  \n",
    "            generated_sequence.append(next_token.squeeze().item())  # Add new token to generated sequence\n",
    "        generated_words = tokens_to_words(generated_sequence)\n",
    "        generation_string = \"\".join(generated_words)\n",
    "        return generation_string\n",
    "\n",
    "# Test generate\n",
    "#model = LanguageTransformerWithReadout(m=32, n=256, L=6, H=8, c=c).to(device)\n",
    "starting_point = torch.randint(0, len(test)-T, (1,))\n",
    "initial_indices = test[starting_point:starting_point+T].unsqueeze(0)\n",
    "print(f\"========== INPUT TEXT ==========\")\n",
    "print(f\"{tokens_to_words(initial_indices.reshape(-1).tolist())}\\n\")\n",
    "\n",
    "# This is the model from task 7\n",
    "print(f\"========== INPUT + GENERATED TEXT ==========\")\n",
    "print(generate_text(estimator, initial_indices, max_generate_tokens=100))\n",
    "print(f\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb1db4-b67d-4244-839c-d0256f3447c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_prompt(estimator,prompt,max_generate_tokens=100):\n",
    "    words = split_to_words(prompt)\n",
    "    tokens = words_to_tokens(words)\n",
    "    token_tensor = torch.tensor(tokens).to(device).unsqueeze(0)\n",
    "    return generate_text(estimator, token_tensor, max_generate_tokens=max_generate_tokens)\n",
    "print(generate_from_prompt(estimator,\"Alas mother,\",max_generate_tokens=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e886feb5-c2c9-494c-8ff3-bc4aa304459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some examples of generated text with the trained model\n",
    "shakespeare_quotes = [\n",
    "    \"All the world's a stage, and all the men and women merely players.\",  # As You Like It (Act 2, Scene 7)\n",
    "    \"A fool thinks himself to be wise, but a wise man knows himself to be a fool.\",  # As You Like It (Act 5, Scene 1)\n",
    "    \"How beauteous mankind is! O brave new world!\",  # The Tempest (Act 5, Scene 1) – Miranda.\n",
    "    \"O brave new world, that has such people in't!\",  # The Tempest (Act 5, Scene 1) – Miranda.\n",
    "    \"Love all, trust a few, do wrong to none.\",  # All's Well That Ends Well (Act 1, Scene 1)\n",
    "    \"To be or not to be, that is the question.\",  # Hamlet (Act 3, Scene 1)\n",
    "]\n",
    "\n",
    "for quote in shakespeare_quotes:\n",
    "    try: \n",
    "        print(f\"========== INPUT ==========\")\n",
    "        print(f\"{' '.join(split_to_words(quote))}\")\n",
    "        print(f\"========== INPUT + GENERATED TEXT ==========\")\n",
    "        print(generate_from_prompt(estimator,quote,max_generate_tokens=15))\n",
    "        print(f\"====================================\")\n",
    "    except Exception as e:\n",
    "        # Some of those words weren't on our vocabulary so the model doesn't know what to do.\n",
    "        print(f\"Error generating from prompt: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72bcc31-8781-45c4-baa9-f45232901614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate positional encoding\n",
    "class LanguageTransformerWithReadoutAndPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Modification of the LanguageTransformerWithReadout class of Task 7 to include positional encoding.\n",
    "    Positional encoding is a learnable matrix that is added to the embeddings of the input tokens.\n",
    "    \n",
    "    Each entry in the positional encoding matrix is is a vector of size n that represents a position in the sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, L, H, c):\n",
    "        super(LanguageTransformerWithReadoutAndPositionalEncoding, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadLayer(m, n, H) for _ in range(L)\n",
    "        ])\n",
    "\n",
    "        # Learnable parameters for positional encoding. \n",
    "        # Each entry in the positional encoding matrix is is a vector of size n that represents a position in the sequence.\n",
    "        self.position_embedding = nn.Embedding(T, n)\n",
    "\n",
    "        self.embedding_table = pca_embeddings\n",
    "\n",
    "        # Adding readout layer\n",
    "        self.readout = nn.Parameter(torch.empty(c, n).to(device))\n",
    "        nn.init.kaiming_uniform_(self.readout, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, E):\n",
    "        \"\"\"\n",
    "        We change the forward pass from the previous Transformer.\n",
    "        Instead of concatenating a vector to the sequence, we now output a vector of probabilities for each word in the sequence.\n",
    "        Args:\n",
    "            E (torch.Tensor): The input word indices.\n",
    "        Returns:\n",
    "            Y_hat (torch.Tensor): The output of the transformer, passed through the readout layer.\n",
    "        \"\"\"\n",
    "        B, T = E.shape\n",
    "\n",
    "        # Word embeddings\n",
    "        X = self.embedding_table[E].transpose(1,2) # (B, n, T)\n",
    "\n",
    "        # To create positional encodings, we need to create a vector for each position in the sequence.\n",
    "        P = self.position_embedding(torch.arange(T, device=device)).transpose(0,1) # (n, T)\n",
    "        \n",
    "        # Adding word embeddings and positional encoding\n",
    "        # Although P is (n,T), this is broadcasted to (B, n, T), which means that the same \n",
    "        # positional encoding is added to every sequence in the batch.\n",
    "        X_tilde = X + P\n",
    "        \n",
    "        # X_l has shape (B, n, T+1)\n",
    "        X_l = X_tilde\n",
    "        for layer in self.layers:\n",
    "            X_l = layer(X_l)\n",
    "\n",
    "        # We implement the readout layer as a linear mapping on each word in the sequence.\n",
    "        Y_hat = torch.matmul(self.readout, X_l) # (B, c, T)\n",
    "\n",
    "        # Notice, we don't apply the softmax here, because we keep the probabilities unnormalized until \n",
    "        # we call the loss function, for numerical stability.\n",
    "        return Y_hat\n",
    "\n",
    "# testing. Now the transformer outputs a vector of probabilities for each word in the sequence.\n",
    "E = torch.randint(0, len(vocab), (1,5)).to(device).long()\n",
    "print(f\"E.shape: {E.shape}\")\n",
    "model = LanguageTransformerWithReadoutAndPositionalEncoding(m=32, n=256, L=6, H=8, c=c).to(device)\n",
    "out = model(E)\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedccc09-d001-4c21-9125-576a3351f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now need to modify both MultiHeadLayer and the LanguageTransformer class to include layer normalization.\n",
    "class MultiHeadLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A modified version of the MultiHeadLayer class with layer normalization.\n",
    "    It will have two normalization layers, one after the multi-head attention and one after the nonlinearity.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, H):\n",
    "        super(MultiHeadLayer, self).__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "\n",
    "        self.Q = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.K = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.V = nn.Parameter(torch.empty(H, m, n))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(n, m))\n",
    "        \n",
    "        # First layer normalization object.\n",
    "        # Layernorm will average over the n dimensions of each element in the sequence.\n",
    "        self.layer_norm1 = nn.LayerNorm(n)\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        \n",
    "        # Second layer normalization object.\n",
    "        self.layer_norm2 = nn.LayerNorm(n)\n",
    "        \n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the values of the learnable parameter matrices.\n",
    "        Kaiming uniform is just a type of random initialization, you don't need to \n",
    "        worry about it. It is a good default initialization for linear layers.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the multihead attention layer with layer normalization.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): The input embeddings.\n",
    "        Returns:\n",
    "            X_l (torch.Tensor): The output of the multihead attention layer.\n",
    "        \"\"\"\n",
    "        B, n, T = X.shape  # X: (B, n, T)\n",
    "\n",
    "        # First layer normalization.\n",
    "        # An annoying Pytorch detail: layer norm function expects the normalization to be over the last dimension.\n",
    "        # Therefore, we need to transpose the last two dimensions of the input to shape (B, T, n) each time we normalize, then transpose back.\n",
    "        # (X.transpose(-2,-1) means that we are transposing over the last two dimensions)\n",
    "        X = self.layer_norm1(X.transpose(-2,-1)).transpose(-2,-1)\n",
    "\n",
    "        # Expand X to include the head dimension\n",
    "        X_expanded = X.unsqueeze(1)  # (B, 1, n, T)\n",
    "\n",
    "        # Compute QX, KX, VX for each head\n",
    "        QX = torch.matmul(self.Q.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        KX = torch.matmul(self.K.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        VX = torch.matmul(self.V.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        \n",
    "        QX_t = QX.transpose(-2, -1)  # (B, H, T, m)\n",
    "\n",
    "        # Compute attention scores B per head\n",
    "        B = torch.matmul(QX_t, KX)  # (B, H, T, T)\n",
    "        A = F.softmax(B, dim=-1)\n",
    "    \n",
    "        A_t = A.transpose(-2,-1)\n",
    "        VXA_t = torch.matmul(VX, A_t) # (B, H, m, T)\n",
    "        Y = torch.matmul(self.W, VXA_t) # (B, H, T, n)\n",
    "\n",
    "        # Second layer normalization. Transpose over the last two dimensions\n",
    "        Y = self.layer_norm2(Y.transpose(-2,-1)).transpose(-2,-1)\n",
    "        \n",
    "        X_l = X + self.nonlinearity(Y.sum(dim=1))\n",
    "\n",
    "        return X_l\n",
    "\n",
    "# Testing the change\n",
    "model = MultiHeadLayer(m=32, n=256, H=2).to(device)\n",
    "X = torch.randn(1,256,5).to(device)\n",
    "out = model(X)\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d8df1-17f1-49d3-b2b6-af3890e0bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Taken from Task 10 and added layer normalization. This is the final version of this class.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, L, H, c, T):\n",
    "        super(LanguageTransformer, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadLayer(m, n, H) for _ in range(L)\n",
    "        ])\n",
    "\n",
    "        # PCA Word embeddings\n",
    "        self.embedding_table = pca_embeddings\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.position_embedding = nn.Embedding(T, n)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(n)\n",
    "\n",
    "        # Adding readout layer\n",
    "        self.readout = nn.Parameter(torch.empty(c, n).to(device))\n",
    "        nn.init.kaiming_uniform_(self.readout, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, E):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            E (torch.Tensor): The input word indices.\n",
    "        Returns:\n",
    "            Y_hat (torch.Tensor): The output of the transformer, passed through the readout layer.\n",
    "        \"\"\"\n",
    "        B, T = E.shape\n",
    "\n",
    "        # Word embeddings\n",
    "        X = self.embedding_table[E].transpose(1,2) # (B, n, T)\n",
    "\n",
    "        # To create positional encodings, we need to create a vector for each position in the sequence.\n",
    "        P = self.position_embedding(torch.arange(T, device=device)).transpose(0,1) # (n, T)\n",
    "        \n",
    "        X_tilde = X + P\n",
    "        \n",
    "        # X_l has shape (B, n, T+1)\n",
    "        X_l = X_tilde\n",
    "        for layer in self.layers:\n",
    "            X_l = layer(X_l)\n",
    "\n",
    "        X_l = self.layer_norm(X_l.transpose(-2,-1)).transpose(-2,-1)\n",
    "\n",
    "        # We implement the readout layer as a linear mapping on each word in the sequence.\n",
    "        Y_hat = torch.matmul(self.readout, X_l) # (B, c, T)\n",
    "\n",
    "        \n",
    "        return Y_hat\n",
    "\n",
    "# testing. \n",
    "E = torch.randint(0, pca_embeddings.shape[0], (1,5)).to(device).long()\n",
    "print(f\"E.shape: {E.shape}\")\n",
    "model = LanguageTransformer(m=32, n=256, L=6, H=8, c=c, T=5).to(device)\n",
    "out = model(E)\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8c088-c4ae-4e5b-9ef3-e47750627c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# future masking\n",
    "# this is a small example to gain an intuition of how masking will work. \n",
    "B = torch.randn(5,5)\n",
    "print(\"B:\")\n",
    "display(B)\n",
    "# FUTURE MASKING: \n",
    "# To mask attention, we create a matrix that indicates if an entry in B is a word in the future\n",
    "mask = torch.triu(torch.ones(T, T), diagonal=1).to(device)\n",
    "print()\n",
    "\n",
    "# If an entry is in the future, we set it to -inf, \n",
    "# so that when we apply softmax, the probability of that word is 0, while \n",
    "# the rest of the words sum to 1.\n",
    "B = B.masked_fill(mask == 1, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6a4ed-684e-41f4-b316-03c36c16cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to modify both MultiHeadLayer and the LanguageTransformer class to include layer normalization.\n",
    "class MultiHeadLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A modified version of the MultiHeadLayer class with layer normalization.\n",
    "    It will have two normalization layers, one after the multi-head attention and one after the nonlinearity.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, H):\n",
    "        super(MultiHeadLayer, self).__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "\n",
    "        self.Q = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.K = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.V = nn.Parameter(torch.empty(H, m, n))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(n, m))\n",
    "        \n",
    "        # First layer normalization object.\n",
    "        # Layernorm will average over the n dimensions of each element in the sequence.\n",
    "        self.layer_norm1 = nn.LayerNorm(n)\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        \n",
    "        # Second layer normalization object.\n",
    "        self.layer_norm2 = nn.LayerNorm(n)\n",
    "        \n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the values of the learnable parameter matrices.\n",
    "        Kaiming uniform is just a type of random initialization, you don't need to \n",
    "        worry about it. It is a good default initialization for linear layers.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the multihead attention layer with layer normalization.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): The input embeddings.\n",
    "        Returns:\n",
    "            X_l (torch.Tensor): The output of the multihead attention layer.\n",
    "        \"\"\"\n",
    "        B, n, T = X.shape  # X: (B, n, T)\n",
    "\n",
    "        # First layer normalization.\n",
    "        # An annoying Pytorch detail: layer norm function expects the normalization to be over the last dimension.\n",
    "        # Therefore, we need to transpose the last two dimensions of the input to shape (B, T, n) each time we normalize, then transpose back.\n",
    "        # (X.transpose(-2,-1) means that we are transposing over the last two dimensions)\n",
    "        X = self.layer_norm1(X.transpose(-2,-1)).transpose(-2,-1)\n",
    "\n",
    "        # Expand X to include the head dimension\n",
    "        X_expanded = X.unsqueeze(1)  # (B, 1, n, T)\n",
    "\n",
    "        # Compute QX, KX, VX for each head\n",
    "        QX = torch.matmul(self.Q.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        KX = torch.matmul(self.K.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        VX = torch.matmul(self.V.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        \n",
    "        QX_t = QX.transpose(-2, -1)  # (B, H, T, m)\n",
    "\n",
    "        # Compute attention scores B per head\n",
    "        B = torch.matmul(QX_t, KX)  # (B, H, T, T)\n",
    "\n",
    "        # FUTURE MASKING: \n",
    "        # To mask attention, we create a matrix that indicates if an entry in B is a word in the future\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).to(device)\n",
    "        \n",
    "        # If an entry is in the future, we set it to -inf, \n",
    "        # so that when we apply softmax, the probability of that word is 0, while \n",
    "        # the rest of the words sum to 1.\n",
    "        B = B.masked_fill(mask == 1, float('-inf'))\n",
    "\n",
    "        # Now when we apply softmax, only the words in the past are have nonzero probability.\n",
    "        A = F.softmax(B, dim=-1)\n",
    "    \n",
    "        A_t = A.transpose(-2,-1)\n",
    "        VXA_t = torch.matmul(VX, A_t) # (B, H, m, T)\n",
    "        Y = torch.matmul(self.W, VXA_t) # (B, H, T, n)\n",
    "\n",
    "        # Second layer normalization. Transpose over the last two dimensions\n",
    "        Y = self.layer_norm2(Y.transpose(-2,-1)).transpose(-2,-1)\n",
    "        \n",
    "        X_l = X + self.nonlinearity(Y.sum(dim=1))\n",
    "\n",
    "        return X_l\n",
    "\n",
    "# Testing the change\n",
    "model = MultiHeadLayer(m=32, n=256, H=2).to(device)\n",
    "X = torch.randn(1,256,5).to(device)\n",
    "out = model(X)\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7046f2e5-6bcc-48ae-83ad-95ceea780b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "n_epochs=5\n",
    "\n",
    "L = 6\n",
    "H = 8\n",
    "m=32\n",
    "n=256\n",
    "lr = 1e-4\n",
    "T=64\n",
    "B=32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "estimator = LanguageTransformer(m=m, n=n, L=L, H=H, c=c,T=T).to(device)\n",
    "estimator.train()\n",
    "\n",
    "# Here we use AdamW instead of SGD. It is just a different optimizer.\n",
    "optimizer = optim.AdamW(estimator.parameters(), lr=lr)\n",
    "\n",
    "# We use the Cross Entropy loss for estimating the probabilities of the next word.\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss = []\n",
    "for epoch in range(n_epochs): # Iterate over n_epochs epochs\n",
    "\n",
    "    for x_batch, y_batch in tqdm(train_loader): # Iterate over all batches in the dataset \n",
    "        # (Step i) Load the data. These commands send the data to the GPU memory.\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        batch_size, T = x_batch.shape\n",
    "\n",
    "        # (Step ii) Compute the gradients. We use automated differentiation.\n",
    "        optimizer.zero_grad() # Gradient reset to indicate where the backward computation stops.\n",
    "\n",
    "        # Call the neural network. In this case, we will take the average of the output of the\n",
    "        # transformer as the prediction.\n",
    "        y_hat = estimator(x_batch)\n",
    "\n",
    "        # Reshape logits and y to be able to evaluate cross entropy on \n",
    "        # each token in the sequence.\n",
    "        y_hat = y_hat.permute(0,2,1)\n",
    "        y_hat = y_hat.reshape(batch_size * T, c)\n",
    "\n",
    "        # Y should also be condensed into one dimension.\n",
    "        y_batch = y_batch.view(batch_size * T, -1).squeeze()\n",
    "\n",
    "        # When using cross entropy loss, we need to pass the target as a 1D tensor of class indices.\n",
    "        # The softmax function is applied internally to the transformer's output y_hat.\n",
    "        cross_entropy_value = cross_entropy_loss(y_hat,y_batch)\n",
    "\n",
    "        cross_entropy_value.backward() # Compute gradients moving backwards untit the gradient reset.\n",
    "\n",
    "        # (Step iii) Update parameters by taking an SGD (or other optimizer) step.\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(cross_entropy_value.item())\n",
    "    print(f\"Epoch {epoch}/{n_epochs} Loss: {train_loss[-1]}\")\n",
    "\n",
    "    # End of batch loop.\n",
    "\n",
    "estimator.eval()\n",
    "with torch.no_grad():\n",
    "    test_losses = []\n",
    "    for x_batch, y_batch in tqdm(test_loader):\n",
    "        x_batch = x_batch.to(device)\n",
    "        batch_size, T = x_batch.shape\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = estimator(x_batch)\n",
    "\n",
    "        y_hat = y_hat.permute(0,2,1)\n",
    "        y_hat = y_hat.reshape(batch_size * T, c)\n",
    "        y_batch = y_batch.view(batch_size * T, -1).squeeze()\n",
    "        cross_entropy_value = cross_entropy_loss(y_hat,y_batch)\n",
    "        test_losses.append(cross_entropy_value.item())\n",
    "    test_loss = torch.tensor(test_losses).mean().item()\n",
    "\n",
    "print(f\"Train loss: {train_loss[-1]}\")\n",
    "print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c3d997-7b5e-4617-8a65-6096c5e93014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeating the generative task\n",
    "initial_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54993172-da84-425f-bd51-cdf829249056",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_point = torch.randint(0, len(test)-T, (1,))\n",
    "# Example sampled from test set.\n",
    "initial_indices = test[starting_point:starting_point+T].unsqueeze(0)\n",
    "print(f\"========== INPUT TEXT ==========\")\n",
    "print(f\"{tokens_to_words(initial_indices.reshape(-1).tolist())}\\n\")\n",
    "\n",
    "# This is the model from task 7\n",
    "print(f\"========== INPUT + GENERATED TEXT ==========\")\n",
    "print(generate_text(estimator, initial_indices, max_generate_tokens=100))\n",
    "print(f\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86457302-3b0f-48cc-965a-6329d7abb048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some examples of generated text with the trained model\n",
    "shakespeare_quotes = [\n",
    "    \"All the world's a stage, and all the men and women merely players.\",  # As You Like It (Act 2, Scene 7)\n",
    "    \"A fool thinks himself to be wise, but a wise man knows himself to be a fool.\",  # As You Like It (Act 5, Scene 1)\n",
    "    \"How beauteous mankind is! O brave new world!\",  # The Tempest (Act 5, Scene 1) – Miranda.\n",
    "    \"O brave new world, that has such people in't!\",  # The Tempest (Act 5, Scene 1) – Miranda.\n",
    "    \"Love all, trust a few, do wrong to none.\",  # All's Well That Ends Well (Act 1, Scene 1)\n",
    "    \"To be or not to be, that is the question.\",  # Hamlet (Act 3, Scene 1)\n",
    "]\n",
    "\n",
    "for quote in shakespeare_quotes:\n",
    "    try: \n",
    "        print(f\"========== INPUT ==========\")\n",
    "        print(f\"{' '.join(split_to_words(quote))}\")\n",
    "        print(f\"========== INPUT + GENERATED TEXT ==========\")\n",
    "        print(generate_from_prompt(estimator,quote,max_generate_tokens=15))\n",
    "        print(f\"====================================\")\n",
    "    except Exception as e:\n",
    "        # Some of those words weren't on our vocabulary so the model doesn't know what to do.\n",
    "        print(f\"Error generating from prompt: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
